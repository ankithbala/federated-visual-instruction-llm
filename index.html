<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Federated Visual Instruction Tuning</title>
  <link rel="stylesheet" href="assets/style.css"/>
</head>
<body>
  <header>
    <h1>Multimodal LLM using Federated Visual Instruction Tuning for Visually Impaired Users</h1>
    <p class="author">Ankith Bala · Alina Vereshchaka</p>
    <p class="conf">ACM ICMI 2025 (Multimodal Interaction Track)</p>
  </header>

  <section class="video">
    <h2>📽️ Presentation Video</h2>
    <iframe width="560" height="315"
        src="https://drive.google.com/file/d/1l83va7ZHVO5y98XsZg5MXsjxWRj2oO5W/view"
        title="Conference Presentation"
        frameborder="0"
        allowfullscreen>
    </iframe>
  </section>

  <section class="links">
    <h2>🔗 Project Links</h2>
    <ul>
      <li><a href="https://dl.acm.org/doi/10.1145/3716553.3750763" target="_blank">📄 Paper (PDF)</a></li>
      <li><a href="https://github.com/ankithbala/federated-visual-instruction-llm" target="_blank">💻 Code Repository</a></li>
    </ul>
  </section>

  <section class="abstract">
    <h2>📘 Abstract</h2>
    <p>
      We propose a novel approach combining visual instruction tuning and federated learning to develop multimodal large language models tailored for visually impaired users. Our system enables privacy-aware adaptation while supporting instruction-following across vision tasks like text reading, object identification, and navigation.
    </p>
  </section>

  <footer>
    <p>© 2025 Ankith Bala</p>
  </footer>
</body>
</html>
